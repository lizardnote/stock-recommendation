# -*- coding: utf-8 -*-
"""Word2vec_and_Clustering

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_qlkdrW2PdZvsiQHc6cgysGdzJ6AdU9d
"""

!pip install finance-datareader

# Commented out IPython magic to ensure Python compatibility.
import matplotlib as mpl
import matplotlib.pyplot as plt # 차트를 그리기 위한 matplotlib을 임포트
 
# %config InlineBackend.figure_format = 'retina'
 
#!apt -qq -y install fonts-nanum
 
import matplotlib.font_manager as fm
fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'
font = fm.FontProperties(fname=fontpath, size=9)
plt.rc('font', family='NanumBarunGothic') 
mpl.font_manager._rebuild()

import FinanceDataReader as fdr
from bs4 import BeautifulSoup
import urllib.request as req
import requests
import numpy as np
from tqdm import tqdm

url = 'https://finance.naver.com/sise/entryJongmok.nhn?&page=1'

r = requests.get(url)
r.content

soup = BeautifulSoup(r.content, "html.parser")
kospi_list = soup.select("td.ctg")

kospi_list

kospi_list[0]

kospi_list[0].a.get('href')

kospi_list[0].a.get('href')[-6:]

"""각 페이지에 10개의 종목이 시가총액 순으로 정렬되어있으므로 20페이지를 수행하면 KOSPI 상위 200개의 종목명과 종목코드를 뽑을 수 있다."""

kospi200_dict = {}
kospi200_list = []
Url = "https://finance.naver.com/sise/entryJongmok.nhn?&page="

# 1페이지부터 20페이지까지에 대해서 아래의 반복문을 반복.
for i in tqdm(range(1,21)):
    url = Url + str(i)
    r = requests.get(url)
    soup = BeautifulSoup(r.content, "html.parser")

    # td 태그의 클래스 값이 ctg인 경우를 전부 추출
    kospi_list = soup.select("td.ctg")
    for name in kospi_list:
        krx = name.a.get('href')[-6:]
        kospi200_dict[krx] = name.string.replace(" ", "")
        kospi200_list.append(krx)

"""각 종목의 3년 간 변화율 값 얻기"""

import pandas as pd

samsung_df = fdr.DataReader('005930', '2019-01-01', '2021-12-31')['Close']

samsung_df

""".pct_change()를 사용해서 종가의 변화율을 얻는다."""

samsung_df = samsung_df.pct_change()
samsung_df

samsung_df = samsung_df.dropna()
samsung_df

samsung_df = pd.DataFrame(samsung_df)
samsung_df

samsung_df.columns.values[0] = 'rate'
samsung_df

"""모든 종목에 대해서 수행하고 하나의 데이터프레임으로 만들어준다."""

def total_record(cprc):
    return (stock_code, cprc)

# tqdm을 원소를 꺼내올 파이썬의 리스트에 tqdm(리스트)로 감싸주시면 얼마나 진행되는지 프로그레스 바를 출력합니다.
data = None
index = 0

for stock_code in tqdm(kospi200_list):
    temp_df = fdr.DataReader(stock_code, '2019-01-01', '2021-12-31')['Close']

    # 만약 종목의 데이터가 없을 경우 확인을 위해서 출력
    if len(temp_df) == 0:
        print(stock_code,'는 데이터가 없습니다.')

    temp_df = temp_df.pct_change()
    temp_df = temp_df.dropna()
    temp_df = pd.DataFrame(temp_df)
    temp_df.columns.values[0] = 'rate'
    temp_df['rate'] = temp_df['rate'].map(lambda x : [total_record(x)])

    if index==0:
        # 맨 처음에는 데이터프레임이 존재하지 않으므로 데이터프레임을 하나 만듭니다.
        data=pd.DataFrame(temp_df.copy())
        index=index+1
    else:
        # 두번째 데이터프레임부터는 기존 데이터프레임에 데이터프레임을 뒤에 계속 추가합니다.
        data=data.append(temp_df)
        index=index+1

len(data)

"""상위 5개의 행은 2019년 1월 3일부터 2019년 1월 9일까지의 (삼성전자, 전날 대비 변화율)"""

data.head()

"""전체 데이터프레임에서 동일한 날짜의 데이터는 전부 하나의 행으로 묶어주는 작업을 한다. 각 날짜의 행에 (종목, 그 종목의 전날 비교 변화율)을 모아놓음."""

train_data=data.groupby(['Date']).sum()
train_data=pd.DataFrame(train_data)
train_data.columns.values[0] = 'rate'
train_data[:5]

"""변화율 순으로 정렬"""

train_data['rate'] = train_data['rate'].map(lambda x : sorted(x, key=lambda x: x[1], reverse=True))

train_data['rate'][:5]

"""정렬된 상황에서 변화율값은 필요없으니 종목코드만 남긴다."""

train_data['rate'] = train_data['rate'].map(lambda x : [i[0] for i in x])

train_data['rate'][:5]

"""train data를 리스트형태로 바꿔준다."""

train_list = train_data.values.tolist()

len(train_list)

print(train_list[0])

"""현재 리스트가 삼중 리스트이므로 이중리스트로 바꿔준다."""

train_list = [item[0] for item in train_list]

len(train_list)

"""이중리스트에 있는 종목 코드들을 앞서 만들어놓은 kospi200_dict를 사용해서 종목 이름으로 바꿔준다."""

print(len(kospi200_dict))

temp2=[]
for line in tqdm(train_list):
  temp1=[]
  for code in line:
    temp1.append(kospi200_dict[code])
  temp2.append(temp1)

len(temp2)

print(temp2[0])

"""생성된 데이터에 대해서 Word2Vec"""

from gensim.models import Word2Vec # 이 데이터에 대해서 워드 임베딩을 해봅시다.
from gensim.models import KeyedVectors

sg_model = Word2Vec(sentences=temp2, size=100, window=3, min_count=5, workers=4, sg=1)

case1=sg_model.wv.most_similar("현대차")
print(case1)

sg_model.wv.save_word2vec_format('stock2vec_skipgram') # 모델 저장

model = KeyedVectors.load_word2vec_format("stock2vec_skipgram") # 모델 로드

case1=model.wv.most_similar("현대차")
print(case1)

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

#mpl.rcParams['axes.unicode_minus'] = False
plt.rc('font', family='NanumBarunGothic') 
# plt.rcParams["font.family"] = "Malgun Gothic"
plt.rcParams["font.size"] = 12
plt.rcParams["figure.figsize"] = (14,4)

def tsne_plot(model):
    labels = []
    tokens = []

    for word in model.wv.vocab:
        tokens.append(model[word])
        labels.append(word)
    
    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)
    new_values = tsne_model.fit_transform(tokens)

    x = []
    y = []
    for value in new_values:
        x.append(value[0])
        y.append(value[1])
        
    plt.figure(figsize=(16, 16)) 
    for i in range(len(x)):
        plt.scatter(x[i],y[i])
        plt.annotate(labels[i],
                     xy=(x[i], y[i]),
                     xytext=(5, 2),
                     textcoords='offset points',
                     ha='right',
                     va='bottom')
    plt.show()

tsne_plot(model)

from sklearn.cluster import KMeans

word_vectors = model.wv.syn0
num_clusters = 5

print(num_clusters)

len(word_vectors)

# Initalize a k-means object and use it to extract centroids
kmeans_clustering = KMeans( n_clusters = num_clusters )
idx = kmeans_clustering.fit_predict( word_vectors )

skipgram_dict = {}

idx = list(idx)
names = model.wv.index2word
word_centroid_map = {names[i]: idx[i] for i in range(len(names))}
#     word_centroid_map = dict(zip( model.wv.index2word, idx ))

# 첫 번째 클러스터의 처음 20개를 출력
for cluster in range(0, num_clusters):
    # 클러스터 번호를 출력
    print("\nCluster {}".format(cluster))

    # 클러스터 번호와 클러스터에 있는 단어를 찍는다.
    words = []
    for i in range(0,len(list(word_centroid_map.values()))):
        if( list(word_centroid_map.values())[i] == cluster ):
            words.append(list(word_centroid_map.keys())[i])
            skipgram_dict[list(word_centroid_map.keys())[i]]=cluster
    print(words)

num_clusters = 7

# Initalize a k-means object and use it to extract centroids
kmeans_clustering = KMeans( n_clusters = num_clusters )
idx = kmeans_clustering.fit_predict( word_vectors )

idx = list(idx)
names = model.wv.index2word
word_centroid_map = {names[i]: idx[i] for i in range(len(names))}
#     word_centroid_map = dict(zip( model.wv.index2word, idx ))

# 첫 번째 클러스터의 처음 20개를 출력
for cluster in range(0, num_clusters):
    # 클러스터 번호를 출력
    print("\nCluster {}".format(cluster))

    # 클러스터 번호와 클러스터에 있는 단어를 찍는다.
    words = []
    for i in range(0,len(list(word_centroid_map.values()))):
        if( list(word_centroid_map.values())[i] == cluster ):
            words.append(list(word_centroid_map.keys())[i])
            skipgram_dict[list(word_centroid_map.keys())[i]]=cluster
    print(words)

from gensim.models import Word2Vec # 이 데이터에 대해서 워드 임베딩을 해봅시다.
from gensim.models import KeyedVectors

cb_model = Word2Vec(sentences=temp2, size=100, window=3, min_count=5, workers=4, sg=0)

case1=cb_model.wv.most_similar("현대차")
print(case1)

case2=cb_model.wv.most_similar("삼성증권")
print(case2)

case3=cb_model.wv.most_similar("롯데케미칼")
print(case3)

case4=cb_model.wv.most_similar("GS건설")
print(case4)

cb_model.wv.save_word2vec_format('stock2vec_cbow') # 모델 저장

case1=model.wv.most_similar("현대차")
print(case1)

tsne_plot(model)

from sklearn.cluster import KMeans

word_vectors = model.wv.syn0
num_clusters = 5

print(num_clusters)

# Initalize a k-means object and use it to extract centroids
kmeans_clustering = KMeans( n_clusters = num_clusters )
idx = kmeans_clustering.fit_predict( word_vectors )

cbow_dict = {}

idx = list(idx)
names = model.wv.index2word
word_centroid_map = {names[i]: idx[i] for i in range(len(names))}
#     word_centroid_map = dict(zip( model.wv.index2word, idx ))

# 첫 번째 클러스터의 처음 20개를 출력
for cluster in range(0, num_clusters):
    # 클러스터 번호를 출력
    print("\nCluster {}".format(cluster))

    # 클러스터 번호와 클러스터에 있는 단어를 찍는다.
    words = []
    for i in range(0,len(list(word_centroid_map.values()))):
        if( list(word_centroid_map.values())[i] == cluster ):
            words.append(list(word_centroid_map.keys())[i])
            cbow_dict[list(word_centroid_map.keys())[i]]=cluster
    print(words)